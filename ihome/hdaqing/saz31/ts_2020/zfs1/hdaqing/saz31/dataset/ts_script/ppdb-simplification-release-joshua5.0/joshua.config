# This file is a template for the Joshua pipeline; variables enclosed
# in <angle-brackets> are substituted by the pipeline script as
# appropriate.  This file also serves to document Joshua's many
# parameters.

# These are the grammar file specifications.  Joshua supports an
# arbitrary number of grammar files, each specified on its own line
# using the following format:
#
#   tm = TYPE OWNER LIMIT FILE
# 
# TYPE is "packed", "thrax", or "samt".  The latter denotes the format
# used in Zollmann and Venugopal's SAMT decoder
# (http://www.cs.cmu.edu/~zollmann/samt/).
# 
# OWNER is the "owner" of the rules in the grammar; this is used to
# determine which set of phrasal features apply to the grammar's
# rules.  Having different owners allows different features to be
# applied to different grammars, and for grammars to share features
# across files.
#
# LIMIT is the maximum input span permitted for the application of
# grammar rules found in the grammar file.  A value of -1 implies no limit.
#
# FILE is the grammar file (or directory when using packed grammars).
# The file can be compressed with gzip, which is determined by the
# presence or absence of a ".gz" file extension.
#
# By a convention defined by Chiang (2007), the grammars are split
# into two files: the main translation grammar containing all the
# learned translation rules, and a glue grammar which supports
# monotonic concatenation of hierarchical phrases. The glue grammar's
# main distinction from the regular grammar is that the span limit
# does not apply to it.  

tm = thrax pt 20 /nlp/users/xwe/experiments/ppdb-simplification/pro-ppdb-1.0-xl-all-simp/data/tune/grammar.packed
tm = thrax pit 20 /nlp/users/xwe/data/ppdb/ppdb-1.0-xxxl-lexical-self-simp.gz
#tm = thrax pt 20 /scratch/users/xwe/scratch/users/xwe/data/ppdb/ppdb-1.0-xl-all
tm = thrax glue -1 /nlp/users/xwe/experiments/ppdb-simplification/pro-ppdb-1.0-xl-all-simp/data/tune/grammar.glue

# This symbol is used over unknown words in the source language

default-non-terminal = X

# This is the goal nonterminal, used to determine when a complete
# parse is found.  It should correspond to the root-level rules in the
# glue grammar.

goal-symbol = GOAL

# Language model config.

# Multiple language models are supported.  For each language model,
# create a line in the following format, 
#
# lm = TYPE 5 false false 100 FILE
#
# where the six fields correspond to the following values:
# - LM type: one of "kenlm", "berkeleylm", "javalm" (not recommended), or "none"
# - LM order: the N of the N-gram language model
# - whether to use left equivalent state (currently not supported)
# - whether to use right equivalent state (currently not supported)
# - the ceiling cost of any n-gram (currently ignored)
# - LM file: the location of the language model file
# You also need to add a weight for each language model below.

lm = kenlm 5 true false 100 /nlp/users/xwe/data/lm/lm-merged.kenlm
#lm = kenlm 5 true false 100 /scratch/users/xwe/experiments/ppdb-simplification/simplification-corpus-2014/PWKP_108016_one2one.penntok.lc.se.train.simp.blm

# The suffix _OOV is appended to unknown source-language words if this
# is set to true.

mark-oovs = false

# The pop-limit for decoding.  This determines how many hypotheses are
# considered over each span of the input.

pop-limit = 100

# How many hypotheses to output

top-n = 300

# Whether those hypotheses should be distinct strings

use-unique-nbest = true

# This is the default format of the ouput printed to STDOUT.  The variables that can be
# substituted are:
#
# %i: the sentence number (0-indexed)
# %s: the translated sentence
# %t: the derivation tree
# %f: the feature string
# %c: the model cost

#output-format = %i ||| %s ||| %t ||| %f ||| %c
output-format = %i ||| %s ||| %f ||| %c

# When printing the trees (%t in 'output-format'), this controls whether the alignments
# are also printed.

include-align-index = false

# And these are the feature functions to activate.
feature-function = OOVPenalty
feature-function = WordPenalty


## Model weights #####################################################

# For each langage model line listed above, create a weight in the
# following format: the keyword "lm", a 0-based index, and the weight.
# lm_INDEX WEIGHT

lm_0 1.0

#lm_0 0.5
#lm_1 0.5

# The phrasal weights correspond to weights stored with each of the
# grammar rules.  The format is
#
#   tm_OWNER_COLUMN WEIGHT


#
# where COLUMN denotes the 0-based order of the parameter in the
# grammar file and WEIGHT is the corresponding weight.  In the future,
# we plan to add a sparse feature representation which will simplify
# this.

Abstract 0.0
Adjacent 0.3413614596362215
CharCountDiff -0.3539660628200161
CharLogCR -0.08171447994105212
ContainsX 0.05632089212520813
GlueRule 0.0
Identity 0.841859597906719
Lex(e|f) -0.017089768607523276
Lex(f|e) 0.003986713693544168
Lexical 0.21497861034369384
LogCount -0.13691707493567465
Monotonic -0.2769873260270465
PhrasePenalty -0.3719878706585434
RarityPenalty 0.6408311839090025
SourceTerminalsButNoTarget 1.650399775776818E-4
SourceWords 0.12040100291572577
TargetTerminalsButNoSource -0.00980896571403925
TargetWords -0.170904799299767
UnalignedSource 0.19010067541451217
UnalignedTarget 0.03976145923269107
WordCountDiff -0.28827869866711653
WordLenDiff -0.12225578334622006
WordLogCR 0.085815090138489
p(LHS|e) 0.19048075029329697
p(LHS|f) -0.1787082328235829
p(e|LHS) -0.09989448694338339
p(e|f) -0.010328292125030015
p(e|f,LHS) -0.12159319506463925
p(f|LHS) 0.10433776323617283
p(f|e) -0.15973867965974453
p(f|e,LHS) 0.07025831622007556
AGigaSim 0.43358561964283643
GoogleNgramSim 1.0820484592360717
SourceMaxWordSyl 0.0
TargetMaxWordSyl 0.0
MaxWordSylDiff -0.5
MaxDiffWordSylDiff -0.5
MaxDiffWordLenDiff -0.5
HardWordDiff -0.5
SourceGigaLM 0.0
TargetGigaLM 0.0
GigaLMDiff 1.0
tm_glue_0 -0.07251819069873632

#SourceSimpLM 0.0
#TargetSimpLM 0.0
#SimpLMDiff 0.0

# The wordpenalty feature counts the number of words in each hypothesis.

WordPenalty 0.12626459781259364

# This feature counts the number of unknown words in the hypothesis.

OOVPenalty -0.026054753290386503

# This feature weights paths through an input lattice.  It is only activated
# when decoding lattices.



