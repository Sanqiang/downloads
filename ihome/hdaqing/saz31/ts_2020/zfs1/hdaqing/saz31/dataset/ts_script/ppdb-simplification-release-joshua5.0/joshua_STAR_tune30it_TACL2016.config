# This file is a template for the Joshua pipeline; variables enclosed
# in <angle-brackets> are substituted by the pipeline script as
# appropriate.  This file also serves to document Joshua's many
# parameters.

# These are the grammar file specifications.  Joshua supports an
# arbitrary number of grammar files, each specified on its own line
# using the following format:
#
#   tm = TYPE OWNER LIMIT FILE
# 
# TYPE is "packed", "thrax", or "samt".  The latter denotes the format
# used in Zollmann and Venugopal's SAMT decoder
# (http://www.cs.cmu.edu/~zollmann/samt/).
# 
# OWNER is the "owner" of the rules in the grammar; this is used to
# determine which set of phrasal features apply to the grammar's
# rules.  Having different owners allows different features to be
# applied to different grammars, and for grammars to share features
# across files.
#
# LIMIT is the maximum input span permitted for the application of
# grammar rules found in the grammar file.  A value of -1 implies no limit.
#
# FILE is the grammar file (or directory when using packed grammars).
# The file can be compressed with gzip, which is determined by the
# presence or absence of a ".gz" file extension.
#
# By a convention defined by Chiang (2007), the grammars are split
# into two files: the main translation grammar containing all the
# learned translation rules, and a glue grammar which supports
# monotonic concatenation of hierarchical phrases. The glue grammar's
# main distinction from the regular grammar is that the span limit
# does not apply to it.  

tm = thrax pt 20 /mnt/nlpgridio2/nlp/users/xwe/experiments/ppdb-simplification/pro-ppdb-1.0-xl-all-simp/data/test/grammar.packed
tm = thrax pit 20 /nlp/users/xwe/data/ppdb/ppdb-1.0-xxxl-lexical-self-simp.gz
#tm = thrax pt 20 /scratch/users/xwe/scratch/users/xwe/data/ppdb/ppdb-1.0-xl-all
tm = thrax glue -1 /nlp/users/xwe/experiments/ppdb-simplification/pro-ppdb-1.0-xl-all-simp/data/tune/grammar.glue

# This symbol is used over unknown words in the source language

default-non-terminal = X

# This is the goal nonterminal, used to determine when a complete
# parse is found.  It should correspond to the root-level rules in the
# glue grammar.

goal-symbol = GOAL

# Language model config.

# Multiple language models are supported.  For each language model,
# create a line in the following format, 
#
# lm = TYPE 5 false false 100 FILE
#
# where the six fields correspond to the following values:
# - LM type: one of "kenlm", "berkeleylm", "javalm" (not recommended), or "none"
# - LM order: the N of the N-gram language model
# - whether to use left equivalent state (currently not supported)
# - whether to use right equivalent state (currently not supported)
# - the ceiling cost of any n-gram (currently ignored)
# - LM file: the location of the language model file
# You also need to add a weight for each language model below.

lm = kenlm 5 true false 100 /nlp/users/xwe/data/lm/lm-merged.kenlm
#lm = kenlm 5 true false 100 /scratch/users/xwe/experiments/ppdb-simplification/simplification-corpus-2014/PWKP_108016_one2one.penntok.lc.se.train.simp.blm

# The suffix _OOV is appended to unknown source-language words if this
# is set to true.

mark-oovs = true

# The pop-limit for decoding.  This determines how many hypotheses are
# considered over each span of the input.

pop-limit = 100

# How many hypotheses to output

top-n = 300

# Whether those hypotheses should be distinct strings

use-unique-nbest = true

# This is the default format of the ouput printed to STDOUT.  The variables that can be
# substituted are:
#
# %i: the sentence number (0-indexed)
# %s: the translated sentence
# %t: the derivation tree
# %f: the feature string
# %c: the model cost

#output-format = %i ||| %s ||| %t ||| %f ||| %c
output-format = %i ||| %s ||| %f ||| %c

# When printing the trees (%t in 'output-format'), this controls whether the alignments
# are also printed.

include-align-index = false

# And these are the feature functions to activate.
feature-function = OOVPenalty
feature-function = WordPenalty


## Model weights #####################################################

# For each langage model line listed above, create a weight in the
# following format: the keyword "lm", a 0-based index, and the weight.
# lm_INDEX WEIGHT

lm_0 0.9999999999999999

#lm_0 0.5
#lm_1 0.5

# The phrasal weights correspond to weights stored with each of the
# grammar rules.  The format is
#
#   tm_OWNER_COLUMN WEIGHT


#
# where COLUMN denotes the 0-based order of the parameter in the
# grammar file and WEIGHT is the corresponding weight.  In the future,
# we plan to add a sparse feature representation which will simplify
# this.

Abstract 0.0
Adjacent 0.2070868767645586
CharCountDiff -0.45747195253816303
CharLogCR 7.097383287718715
ContainsX 0.3429124718563617
GlueRule 0.0
Identity 0.8948041053122943
Lex(e|f) 0.09378785467104021
Lex(f|e) -0.10264119950823311
Lexical -0.8810280865998698
LogCount -0.12667749274527046
Monotonic 2.052074125316996
PhrasePenalty -3.712929728932982
RarityPenalty 1.8204670640185494
SourceTerminalsButNoTarget 6.155014758732219E-4
SourceWords 2.9865702149976654
TargetTerminalsButNoSource 0.10380531327323664
TargetWords 2.076350705083794
UnalignedSource 0.1342282569472083
UnalignedTarget 0.26001165724701275
WordCountDiff -0.7468162641438286
WordLenDiff -1.4508352480715339
WordLogCR -1.2195514974468202
p(LHS|e) -0.05418239419834677
p(LHS|f) 0.12332941894772309
p(e|LHS) -0.1111074205455865
p(e|f) -0.6554268775966806
p(e|f,LHS) 0.11454294676315607
p(f|LHS) 0.06997986801663918
p(f|e) -0.07493110006079233
p(f|e,LHS) -0.2554388944723409
AGigaSim 4.04301272935951
GoogleNgramSim -0.8694672883685616
SourceMaxWordSyl -0.11871712243488021
TargetMaxWordSyl 0.31824959349491294
MaxWordSylDiff 0.422871873835129
MaxDiffWordSylDiff -1.4179421493793232
MaxDiffWordLenDiff 1.765538953710059
HardWordDiff -0.5543581424559411
SourceGigaLM -0.23807524872420505
TargetGigaLM 0.40248570710114545
GigaLMDiff 0.6439185353889383
tm_glue_0 1.2978727553359752

#SourceSimpLM 0.0
#TargetSimpLM 0.0
#SimpLMDiff 0.0

# The wordpenalty feature counts the number of words in each hypothesis.

WordPenalty 0.3957301743578882

# This feature counts the number of unknown words in the hypothesis.

OOVPenalty -1.5473422110951494

# This feature weights paths through an input lattice.  It is only activated
# when decoding lattices.



